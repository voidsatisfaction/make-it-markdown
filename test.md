# 人工知能と社会メモ　2016.11.02

### 質問票への回答のための資料

1.  特徴量とは？
    *   データのもつ特徴をベクトルのような「量」であらわしたもの。　[参考１](http://www.souya.biz/業務システム/機械学習・ディープラーニング/)　[参考２](http://blog.jubat.us/2012/08/blog-post_27.html)

### 前回までの資料　（少し改良）

## AIのタイプ

ディープラーニング、深層学習という言葉を聞くことがあると思う。これは機械学習と呼ばれるAIの一分野の話。ただ、機械学習の専門家は、それを、あまり人工知能と呼ばなかったりするが、これは人工知能学会の重要な分野の一つだから、まぎれもない人工知能だろう。

それから、[Apple の Siri](http://www.apple.com/jp/ios/siri/) 　[＊](https://ja.wikipedia.org/wiki/Siri)　のような、声で質問すると答えてくれるツールに使われている技術は、音声の認識、文の認識、答えの「探索」、回答文の作成、機械による発話、という複数の異なる技術が使われているが、こういうものを全体として[自然言語処理](https://ja.wikipedia.org/wiki/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86)というが、答えの「探索」の部分は、自然言語処理とは言わない。

ここまででも、機械学習、自然言語処理という二つの異なる技術が出てきた。AIのタイプは、どれくらいあるのだろうか？

もちろん、その答は、人により、まちまちで、この人などは[少なくとも３３ある](http://simplicable.com/new/types-of-artificial-intelligence)としている。

ただ、これはちょっと細かすぎるし、最近話題のAIと関係がないものも多い。

現在、話題になっているAIの種類、あるいはAIの技術の種類は、大雑把に言って、

1.  自然言語処理
2.  機械学習、特にニューラルネットワークによるもの
3.  大規模データからの情報抽出

の三つであるといえる。三番目のものは、[データマイニング](https://ja.wikipedia.org/wiki/%E3%83%87%E3%83%BC%E3%82%BF%E3%83%9E%E3%82%A4%E3%83%8B%E3%83%B3%E3%82%B0)とか[ビッグデータ](https://ja.wikipedia.org/wiki/ビッグデータ)などのキーワードで、マスコミなどで見かけるものだと思えばよい。

先に挙げた、日本で有名な三つのAIの事例は、次のように、これらに対応する。

1.  自然言語処理：　IBM Watson, 東ロボ君
2.  機械学習：　AlphaGo
3.  大規模データからの情報抽出：　IBM　Watson

この内、自然言語処理は、IBM Watson, 東ロボ君においては、インターフェース、特に入力の部分で重要な技術だが、これが、これらのコアの部分というわけではない。この自然言語処理、特に、発話された言葉や書かれた言葉を認識する技術の進歩は、現在のAIブームの基盤というか、必要な前提、あるいは、縁の下の力持ち、とでもいうべき重要な技術であるが、これは、すでに、当然のもの、McCarthy が言った　As soon as it works, no one calls it AI any more.　の、it works のレベルに近づきつつある。つまり、もう、それは人工知能と呼ばれる域を脱しつつある。だから、特に、それにより人工知能が最近注目されているわけではなく、注目が集まっているのは、実は、２の機械学習、特に、デープラーニングのような、人間の脳神経のシミュレーションから始まったニューラルネットワークと呼ばれる技術の進化と、３の大規模データからの情報抽出であるといえる。

*   ちなみに、人工知能の一種ともみなせる自動翻訳は、英語とドイツ語の様な、もともと近い言語の間では、ほぼ実用に近づきつつある。しかし、これは言語処理と呼ばれ、なぜか、あまり、人工知能と呼ばれない。こういうのは、カナ漢字変換が自人工知能とみなされなくなったように、「当たり前のもの」「知能と関係ないもの」、McCarthy がいう、 no one calls it AI any more　の it になりつつある。

以上の説明からすると、東ロボ君のAIとしての本体が、最近流行のものではないことになる。実際は、東ロボ君は、ひとつのシステムではなくて、問題文を認識するという所以外は、数学、英語、など、それぞれの科目の問題を解くことに特化されて作られたコンピュータシステムの集まりである。その中で、国語などは人工知能と言って専門家も違和感を持たないだろいうものを使っているらしいが、数学のある種の問題を解くためには、[Quantifier elimination](https://en.wikipedia.org/wiki/Quantifier_elimination)　というアルゴリズムが使われている。株のアルゴリズム取引が、AIによる取引とみなされないことが多いように、人によっては、このアルゴリズムはAIだとみなさないことがあり、AIの専門家には、東ロボ君に批判的あるいは冷淡な人いるのは、このため。しかし、プロジェクトのもともとの目的が、AIというより、コンピュータに仕事が奪われる、という警鐘をならすものだったことからしたら、当然。

この講義では、現在、世界的に流行の２と３のタイプのAIを中心に、しかし、「人から仕事を奪う」可能性を持つという点では、まったく変わらない、東ロボ君のようなシステムも含めて考えていくことにする。

そこで、まず、簡単に、２の機械学習、特にニューラルネットワークによる学習と、３の大規模データからの情報抽出の大雑把な説明をしておく。

## 機械学習

まず、Wikipediaを見てみる。実は、IT分野の話題では、Wikipedia の情報の信用度は非常に高い。一方で、IT分野の書籍、すくなくとも日本語の書籍のレベルは低いので、たとえば、英語の Wikipedia と、日本語のIT関係本を比較すると、英語の Wikipedia の方が、はるかにレベルが高く信頼性が高いことが多い。これは、英語という巨大なユーザ集団をもつ言語で書かれた Wikipedia の文章が、世界中のIT関係者により「監視」されている一方で、日本語のIT関係本、特に非専門家向けのものが、かならずしもITの専門家ではないライターたちにより書かれる、また、書かれたら容易に直せない、ということを考えれば、当然のこと。

*   実は、IT関係の新聞記事が、ほとんど嘘ばかりというのは、専門家の間では有名なこと。これは日本の記者たちが文系の人で、ITを全く理解できていなかったため。ただし、世代交代が起きたためか、最近は改善されている。
*   新聞に書いてあることだからと信用してはいけない。技術関係のことでいえば、技術者たち、理系の人たちが書いている Wikipedia の信頼性の方がはるかに高い。

ということで、話を本題にもどして、機械学習の Wikipedia 記事。

*   [日本語](https://ja.wikipedia.org/wiki/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92)
*   [英語](https://en.wikipedia.org/wiki/Machine_learning)

読んでもピンとこないかもしれないので、実例で説明。

### Googleとキュウリ農家

これは夏に、Google 日本支社を訪問してGoogle のAIへの取り組みを取材した際に、担当者が特に強調していた事例。

元技術者のキュウリ農家の小池誠さんが、Google の機械学習用のツールを利用して、キュウリの自動仕分け機械を作ったという話。

[これは、Google の機械学習を広報するサイトで読める。](http://googlecloudplatform-japan.blogspot.jp/2016/08/tensorflow_5.html)

これの中ほどにある、２LからCまで、さまざまな等級に分けられたキュウリの写真があるが、これが機械学習。

分類は小池さんのお母さんのスキルで、写真を等級に分けて（それはお母さんが行う）、それを Google の TensorFlow というもので学習させた。

そうすると、その等級の意味を、TensorFlow　が学習し、自分で、分類ができるようになったということ。

TensorFlow を新人の手伝いと考えれば、その意味がわかるだろう。

こういうのが機械学習。

### ここから今回の資料

前回、機械学習というものを、その例である、キュウリの等級分けを例に見た。

[機械学習](https://ja.wikipedia.org/wiki/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92)には、非常に多くの異なる方法がある：　Wikipedia [英語](https://en.wikipedia.org/wiki/Machine_learning#Approaches)　[日本語](https://ja.wikipedia.org/wiki/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92#.E6.8A.80.E6.B3.95)

このキュウリの投球分けに使われていたのがディープラーニングという方法で、機械学習の内、最近もっとも注目されているもの。

これの仕組みを説明するには数学が必要で難しくなるので、入門である今はさける。（後で、もしかしたらやる。）

ポイントは、[ここ](http://matome.naver.jp/odai/2140635573608360401)で説明されている「従来職人技だった特徴量抽出を教師なし学習でやってしまうという点で画期的です。これにより、特徴量の設計が難しいがゆえに機械学習でうまく扱えなかった問題を改善できるかもしれません。」という点。

つまり、キュウリのある等級は、これくらいの大きさで、これくらいの曲がり具合で、…、などといういくつかのキュウリの特徴で決まっているはずなのだが、その特徴を、人間でなくて、機械が勝手に学習してくれるということ。

たとえば、[上のページ](http://matome.naver.jp/odai/2140635573608360401)に「ニューロンの一つが「猫」を認識」と書いてあるが、その意味は、[http://d.hatena.ne.jp/Zellij/20130608/p1](http://d.hatena.ne.jp/Zellij/20130608/p1)に解説してあるように、機械が自分で「これらは似ている」という画像の分類法（特徴量）を多数作り出したが、そのうちの一つが「猫の画像」と、ほぼ一致していたということ。

この「猫の認識」の実験はグーグルでおこなれた実験で、報道ではよく、

![20130608212518](http://cdn-ak.f.st-hatena.com/images/fotolife/Z/Zellij/20130608/20130608212518.png)

という画像とともに説明されていて、あたかもグーグルのコンピュータが、たくさんの画像を一人で見て、その後、「これが猫です」と言って、この画像をプリントしたかのように説明されることがあるが、そうではなくて、グーグルのコンピュータが無数に作り出した、画像の特徴量（画像判別方法）の一つに、猫の画像に非常によく反応するものがあった、ということ。また、この画像は、その特徴量を一般の人にわかりやすくするために、その特徴量から人間が作り出したもの。

機械が作り出した一つの特徴量が「猫の画像の特徴量といってよい」という判断や画像の作成は人間がやっていることに注意。

これに反して、最近、碁の一流プレイヤーを破った [AlphaGo](https://ja.wikipedia.org/wiki/AlphaGo) は、最初は人間から学んだが、[そののち、自分自身と対戦して強くなった](https://en.wikipedia.org/wiki/AlphaGo)。これとキュウリの等級分け、猫の画像の認識、の違いに、AIと未来の労働、つまり、どんな職が機械に奪われるのかのヒントが含まれているので、後で再論。

### ディープラーニングは本当に画期的か？

現在の報道の多くは、「ディープラーニングというものができて、それを使うとAIが現実になる」というように読める論調のものが多い。

つまり、ディープラーニングという技術革命が起きて、それにより、ディープラーニング前とディープラーニング後で、世界がガラリと変わりつつある、という風に理解されているのではないかと思える報道が多い。

実は、ディープラーニングブームが巻き起こる前に、すでに機械学習は、いろいろな場面で実用化されている。

そして、そういう、さまざまな努力の総体により、AIが大きな社会的影響を引き起こしそうな状況が生まれている。

だから、あまりに、ディープラーニングにスポットライトをあてると、人工知能の社会影響をかんがえるときには、間違いのもととなる。

たとえば、次に、ディープラーニングとともに、今日のAIブームの火付け役になったと思われる、IBMの人工知能、アメリカの人気クイズ番組で、歴代チャンピオンに買った Watson の話をするが、Waton は、ディープラーニングと関係がない。

また、先に説明したように、IBMは、Watson のようなものをAIとさえ呼ばない。

日本では Watson とディープラーニングと人工知能が、ほぼ同時に話題になったように見えるが、実際には、1990年代から人工知能技術は現実的で、また、WWWの巨大データを処理するために必要な技術になっており、また、アメリカでは、ディープラーニング騒ぎの前に Watson 騒ぎがあった。Watson が、クイズ番組 [Jeoperdy!](https://ja.wikipedia.org/wiki/%E3%82%B8%E3%82%A7%E3%83%91%E3%83%87%E3%82%A3!) でチャンピオンになったのが2011年。これに対し、[デープラーニングが評判になり始めたのは2012年](http://business.nikkeibp.co.jp/article/bigdata/20150419/280107/?ST=print)。しかも、先に説明したように、この二つには直接の関係がない。

そこで、Watsonの話をする前に、どの様な経緯で、現在の様なAIブームが起きたのか、その歴史を簡単に見ておく。

## AIの歴史

今のAIブームは、三回目のブームだと言われることが多い。最初のブームは、1950年代、AIというものが生まれたとき、第2次ブームは1980年代だということで大抵の見方は一致している。

このブームというのは、例えば、アメリカ、日本、ヨーロッパなどで、政府や産業が、大量の研究費・開発費をAIに落とした時期のことを言う。そして、ブームとブームの間には、「冬の季節」があり、それは、AIに投入される資金が大幅に減らされた時期を意味する。

ところが、第二次ブームとは何だったのか、というのは、説明がいろいろある。現在のブームが Watson的なものと、ディープラーニング的なもののごった煮であるように、この時代の主役は一つではなく、次の三つのものの混交だった：

1.  [日本の通産省の新世代コンピュータ（第5世代コンピュータ）プロジェクト](https://ja.wikipedia.org/wiki/第五世代コンピュータ)
2.  [エキスパート・システム](https://ja.wikipedia.org/wiki/エキスパートシステム)
3.  [コネクショニズムの再評価](https://ja.wikipedia.org/wiki/%E3%82%B3%E3%83%8D%E3%82%AF%E3%82%B7%E3%83%A7%E3%83%8B%E3%82%BA%E3%83%A0)

ただし、混交とかごった煮というのは、この三つが融合していたというのでなくて、それぞれ、別のもの、ある意味ではライバルとして存在しており、それらの全体で「（第2次）人工知能ブーム」だったという意味。

そして、これの１，２が、Watson に、３がディープラーニングに対応する。

そのことを念頭において、いくつかの「歴史」を見てみる。

日本語

*   [日本人工知能学会による歴史](https://www.ai-gakkai.or.jp/whatsai/AIhistory.html)
*   [ITソリューション塾](http://blogs.itmedia.co.jp/itsolutionjuku/2015/07/post_105.html)
*   [weblio](http://www.weblio.jp/content/人工知能の歴史) [weblio とは](https://ja.wikipedia.org/wiki/Weblio)
*   [Wikipedia](https://ja.wikipedia.org/wiki/人工知能の歴史)

英語

*   [Wikipedia](https://ja.wikipedia.org/wiki/人工知能の歴史)
*   [AI Topics](http://aitopics.org/misc/brief-history) [AAAI](https://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%A1%E3%83%AA%E3%82%AB%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD%E5%AD%A6%E4%BC%9A) のサイト　AAAIは世界最大のAIの学会。その意味では一番信頼がおける歴史かもしれない。林も、1980年代にAAAIに一度だけ参加したことがある。すごく規模が大きく、また、数十名の報道関係者がうろうろしていて驚いた。

以上の様に、人によりAIの歴史の見方は、異なり、特に、なぜ、第3次AIブームが起きたか、これは本物か、ということの説明は、人により大きく異なる。

これには

*   人工知能研究とは、人間の知能を人工的に作り出しことである。
*   そして、それにより、知能とは何かを知ることである。

という立場と、

*   人工知能とは、人間の知的労働を肩代わりしたり、補助したりするものである。
*   それは機能すればよいのであって、自動車と動物の移動方法が全く違うように、人間の知能とは関係がなくてもよい。

という立場の様に、AIに対する異なる立場があること。

それから、人工知能で実現されるものが、大雑把に：

1.  人間の論理的・合理的判断能力…考える能力、推論力
2.  人間の視覚、聴覚などの感覚による認識能力…直感力

の二つに分類できることにある。

ディープラーニングは、主に２であり、Watson は、１である。

*   ただし、AlphaGo のように、ディープラーニングは１にも応用されるようになってきている。
*   しかし、それは性能を上げるために使われており、ディープラーニングの様なものだけで１を行うことは不可能。
*   ディープラーニングなどの学習アルゴリズムは、本質的に間違えることがある。その間違いを正していくことが「学習」だからである。
*   一方、１の方は、データの誤解釈などで間違える、データが間違えている、などで、間違うこともあるが、基本的には、「論理」の能力なので「間違えない」のが基本。

こう考えると

*   第2次AIブームは、主に「人間の論理的・合理的判断能力…考える能力、推論力」を行うAIが脚光を浴び、
*   現代のAIブームは、2016年の今では、あえていえば「人間の視覚、聴覚などの感覚による認識能力…直感力」を行うAIが脚光を浴びている

といえる。

しかし、AIが職を奪う、というような問題を考えるとき、より重要なのは、前者である。

ただし、先に述べたように、それを実現する際に、AlphaGo の様に、ディープラーニングのような「人間の直観力を代替する認識装置」が使われるようになってきている、といえる。

つまり、人間から職を奪う、特に頭脳労働を行う職、たとえば、弁護士、会計士、医師、教師、ホワイトカラーなどの職を奪う可能性があるのは、AlphaGo 的なもので補強された Watson 風のシステム、だろうと考えられる。

そこで、その Watson の話。

## IBM Watson

IBM では、昔からAIの研究が続けられており、大きな成果を上げてきた。

たとえば、1996, 1997年に、当時のチェスの世界チャンピオン（グランドマスター）[ガリル・カスパロフ](https://ja.wikipedia.org/wiki/%E3%82%AC%E3%83%AB%E3%83%AA%E3%83%BB%E3%82%AB%E3%82%B9%E3%83%91%E3%83%AD%E3%83%95)を破ったスーパーコンピュータ [Deep Blue](https://ja.wikipedia.org/wiki/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%BB%E3%83%96%E3%83%AB%E3%83%BC_(%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF)#.E6.A6.82.E8.A6.81) は、その一つ。

*   上のカスパロフのWIKIPEDIA記事に、「西欧では[チェス](https://ja.wikipedia.org/wiki/%E3%83%81%E3%82%A7%E3%82%B9 "チェス")は理知的なゲームの代表、「人間の知性の極み」と見なされており[[1]](https://ja.wikipedia.org/wiki/%E3%82%AC%E3%83%AB%E3%83%AA%E3%83%BB%E3%82%AB%E3%82%B9%E3%83%91%E3%83%AD%E3%83%95#cite_note-kettei-1)、チェス人口が多いので、カスパロフは頭の良い人物の代表として、しばしば言及される人物でもある。」というフレーズがあることに注意。
*   また、20年後の現在では、チェスより複雑な将棋の世界で、[三浦弘行九段が対局中に将棋ソフトを使用したのではないかと疑われている](http://mainichi.jp/articles/20161026/k00/00m/040/081000c)ことにも注意。

そして、この Deep Blue のプロジェクトの後継プロジェクトが、アメリカのクイズTV番組で、一番といってもよい人気を誇る**[Jeopardy!](https://ja.wikipedia.org/wiki/%E3%82%B8%E3%82%A7%E3%83%91%E3%83%87%E3%82%A3!)**でチャンピオンに勝つことを目的とした、[IBM　Watson](https://ja.wikipedia.org/wiki/ワトソン_(コンピュータ)) のプロジェクトだった。

まず、Jeorpady! がどんなクイズかをみる。[https://www.youtube.com/watch?v=pFhSKPOF_lI](https://www.youtube.com/watch?v=pFhSKPOF_lI)

[Wikipedia の解説](https://ja.wikipedia.org/wiki/ジェパディ!)：　出題と解答の文の形が通常のクイズと反対になっている（肯定文で出題し、疑問文で答える）のを除けば、通常の「知識量を競う」クイズ。

[そして、WatsonのJeorpady!](https://www.youtube.com/watch?v=KVM6KKRa12g)

Jeorpady! において示された Watson の機能とは「英語で質問すると、それに答える」ことであった。

*   注. 現在の Watson の機能は、この機能だけではない。

この機能は、第2次ブームの際の、英語や日本で質問できるというエキスパートシステムと同じ。

それは、次のことからわかる。

第2次ブームで、エキスパート・システムの可能を最初に示したのが、スタンフォード大学の Mycin というシステムだった。

[Mycin](https://ja.wikipedia.org/wiki/Mycin) : 患者の症状から、原因の病原菌・ウィルスを推測し、治療に適当な抗生物質を示唆するシステム。

[白血病を診断した Watson](http://healthpress.jp/2016/08/ai-10ibm-watson_2.html)

この二つが酷似していることがわかる。

ただし、Watson と、Mycin には、いくつかの大きな違いがある。

### 違い１　質問方法

Mycin では、Wikipedia の記事にあるように、症状の入力には、 Yes/No などの選択肢で行われた。つまり、Web でアンケートに答えるような感じ。

Watson では、、これが自然言語（英語や日本語）で行える。

自然言語を機械が、かなり「理解」できる様になったということが、第2次ブームの時代と、現在の非常に大きな違いといえる。

ただし、「理解」というように、理科に括弧「」がついていることに注意してほしい。

### Watson の「理解」と人間の「理解」

これは、機械が、我々人間が行う「文章を読んで、その内容・意味を理解し、文章から情報を得る」というような作業と同じことができるようになったということではない。

Jeorpady!の様なクイズのチャンピオンは、クイズでは、ほとんどの大学教授より、良い成績を残せるはずだが、それだからと言ってクイズのチャンピオンは、大学教授にはなれない。高校の先生にさえなれないはず。

たとえば、「大政奉還をした徳川将軍は？」というような、ある明白な歴史イベントの発生年を聞くような問には、クイズのチャンピオンや Watson は難なく答えるだろうが、

「300年も続いた徳川幕府が大政奉還により、その権力を放棄することになった理由を述べよ」とか「大政奉還における徳川幕府と朝廷の関係を説明せよ」という問題には、

Watson は答える能力は、まだないはず。

また、クイズに問われる様な「雑学的知識」では、大学の歴史学の教授をしのぐ知識を持つであろうクイズ・チャンピオンは、

これらの問題に答えることができるはずだが、その答えは、近世日本史の教授の答えには及ばないだろう。

また、Watson に、数学の問題を聞いたら、非常に単純な問題でも、<span class="red">多分</span>、答えられない。（多分と書いたのは、簡単な問題の場合、特に、センター入試風の問題では、これができるようにするのは、技術的には、それほど難しくはない筈だから。）

知力には、さまざま種類がある。

Watson や、エキスパート・システムの持つ知力というのは、基本的には、これらのシステムがデータベースの中に持つ情報を組み合わせて、推論を行うことである。

要するには、推論マシン、[推論エンジン](https://ja.wikipedia.org/wiki/%E6%8E%A8%E8%AB%96%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%B3)である。Mycin の推論エンジンは、実にシンプルなものであって、基本的には、いわゆる[三段論法](https://ja.wikipedia.org/wiki/%E4%B8%89%E6%AE%B5%E8%AB%96%E6%B3%95)の一番代表的なケースである

AならばB

BならばC

よって

AならばC

を自動的にやっているのだと思えばよい。参考： [Wikipedia Inference Engine （推論エンジン）の Architecture](https://en.wikipedia.org/wiki/Inference_engine#Architecture).

三段論法のようなものを形式論理学というが、そこにおける形式的推論は、意味を理解しないでもできる機械的なもの。

だから、それを使って機械であるコンピュータに推論をさせようということになっのがエキスパート・システム。

しかし、人間は、多くの場合、形式的推論の前に、あるいは、他に、直観とか経験による判断を使う。

Deep Learning などのニューラルネットワークによるAIでは、それに類するものができるのだが、エキスパートシステムにはそれがなかった。

また、Watson では、それを取り込もうとしている兆候が見えるのだが、おそらくは、まだ中心的役割は果たしていない。

### 違い２　巨大なデータとその獲得方法の存在

第3次ブーム以前から、AIを研究しているベテランのAI研究者の間では、第3次ブームと第2次ブームのAIには、

差がなく、特に新しい技術が生まれたのではなくて、単に「ビッグデータ」あるいは巨大データが得られるようになっただけだ、というシニカルな見方をする人が結構多い。

たとえば、Watson の開発に従事した日本人として有名な、[日本IBMの武田浩一さんも、同じような意見](https://enterprisezine.jp/iti/detail/6436)を言っている。

*   ただし、これは第3次ブームにシニカルなのではなくて、巨大なデータの存在が本質的だったのだという意見。

これは、AIを技術面だけからみれば、一理ある意見なのだが、AIと社会との関係を考えるときには、「巨大データ」の存在と、

それを活用できるようにした、自然言語のテキストから情報を得る技術が開発されたことが大きい。

そして、もうひとつ、エキスパートシステムでは、存在はしたが、まだ、弱かった「統計的な思考法」とでもいうべき、

「少数のエキスパートが持つ厳密だけが役に立つのではなくて、むしろ、曖昧さを含む大量の社会的知識の方が、むしろ大きな力になる」

という、WEBの時代が生み出した、知識に関するパラダイム・チェンジが大きい。

はっきり語られることはないのだが、その情報の収集方法や推論方式などを見ると、Watson は、明らかに、この哲学をもとに作られている。

そして、そのパラダイム・チェンジとは、以前、話題になった集合的知識というもの。

ということで、次に、これを説明。

### 集合的知識 Collective Knowledge

[集合的知識](https://ja.wikipedia.org/wiki/集団的知性)とは？（この Wikipedia 記事では、集合的知性になっているが、知識の方がよい！）